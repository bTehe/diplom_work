{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bca2d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import OrderedDict, Counter\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import (FunctionTransformer, PowerTransformer,\n",
    "                                   QuantileTransformer, RobustScaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c01bb9",
   "metadata": {},
   "source": [
    "Цей модуль реалізує конвеєр обробки мережевих даних для виявлення вторгнень. Включає числові трансформації, бінування ознак, відображення міток, розбиття даних на набори, збалансування SMOTE та експорт у CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf87416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Конфігураційні константи\n",
    "DATA_DIR = Path(\"../Dataset\")\n",
    "OUTPUT_DIR = Path(\"../Filtered datasets\")\n",
    "\n",
    "# мапування днів неділі в timestamp\n",
    "DATE_MAP: Dict[str, str] = {\n",
    "    'Monday': '2023-11-06 12:00:00',\n",
    "    'Tuesday': '2023-11-07 12:00:00',\n",
    "    'Wednesday': '2023-11-08 12:00:00',\n",
    "    'Thursday-Morning': '2023-11-09 09:00:00',\n",
    "    'Thursday-Afternoon': '2023-11-09 15:00:00',\n",
    "    'Friday-Morning': '2023-11-10 09:00:00',\n",
    "    'Friday-Afternoon1': '2023-11-10 13:00:00',\n",
    "    'Friday-Afternoon2': '2023-11-10 17:00:00',\n",
    "}\n",
    "\n",
    "CATEGORY_LABELS: Dict[str, List[str]] = {\n",
    "    'BENIGN': ['BENIGN'],\n",
    "    'DoS': ['DDoS', 'DoS slowloris', 'DoS Hulk', 'DoS GoldenEye', 'DoS Slowhttptest'],\n",
    "    'PortScan': ['PortScan'],\n",
    "    'Bot_Infiltration': ['Bot', 'Infiltration'],\n",
    "    'Web': ['Web Attack – Brute Force', 'Web Attack – XSS', 'Web Attack – Sql Injection'],\n",
    "    'FTP_SSH_Patator': ['FTP-Patator', 'SSH-Patator'],\n",
    "    'Heartbleed': ['Heartbleed'],\n",
    "}\n",
    "\n",
    "GROUP_FEATURES: Dict[str, List[str]] = {\n",
    "    'dos': ['Fwd Packets/s', 'Bwd Packets/s', 'Flow Duration', 'Flow IAT Min', 'Flow IAT Max', 'SYN Flag Count', 'PSH Flag Count'],\n",
    "    'portscan': ['SYN Flag Count', 'FIN Flag Count', 'RST Flag Count', 'Total Fwd Packets', 'Total Backward Packets'],\n",
    "    'bot_infiltration': ['Flow Duration', 'Fwd IAT Std', 'Bwd IAT Std', 'Fwd PSH Flags', 'Bwd URG Flags', 'Down/Up Ratio'],\n",
    "    'web': ['Fwd Header Length', 'Bwd Header Length', 'Packet Length Variance', 'ACK Flag Count', 'Average Packet Size'],\n",
    "    'ftp_ssh_patator': ['Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Bwd Avg Bytes/Bulk', 'Active Mean', 'Idle Mean'],\n",
    "    'heartbleed': ['Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd IAT Min', 'Total Length of Fwd Packets', 'Packet Length Std'],\n",
    "}\n",
    "\n",
    "BASE_FEATURES: List[str] = [\n",
    "    'Flow Bytes/s', 'Flow Packets/s', 'Average Packet Size', 'Down/Up Ratio',\n",
    "    'Packet Length Mean', 'Packet Length Std', 'Min Packet Length', 'Max Packet Length',\n",
    "    'Flow IAT Mean', 'Flow IAT Std', 'Fwd IAT Mean', 'Bwd IAT Mean',\n",
    "    'SYN Flag Count', 'FIN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
    "    'Active Mean', 'Idle Mean', 'Subflow Fwd Packets', 'Subflow Bwd Packets',\n",
    "    'Label', 'dow', 'hour', 'time'\n",
    "]\n",
    "\n",
    "STD_FEATURES: List[str] = [\n",
    "    'Fwd Packet Length Std', 'Bwd Packet Length Std', 'Flow IAT Std',\n",
    "    'Fwd IAT Std', 'Bwd IAT Std', 'Packet Length Std', 'Active Std', 'Idle Std'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de71cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_concat_csvs(data_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Завантажує всі CSV-файли з директорії, додає стовпець 'Day' на основі імені файлу\n",
    "    та об'єднує їх в один DataFrame.\n",
    "\n",
    "    :param data_dir: шлях до директорії з CSV-файлами\n",
    "    :return: конкатенований DataFrame з сирими даними\n",
    "    \"\"\"\n",
    "    csv_paths = sorted(data_dir.glob(\"*.csv\"))\n",
    "    dfs: List[pd.DataFrame] = []\n",
    "\n",
    "    for path in csv_paths:\n",
    "        day_label = path.stem  # мітка дня з імені файлу\n",
    "        df_temp = pd.read_csv(path)\n",
    "        df_temp['Day'] = day_label\n",
    "        dfs.append(df_temp)\n",
    "\n",
    "    concatenated = pd.concat(dfs, ignore_index=True)\n",
    "    concatenated.columns = concatenated.columns.str.strip()  # очищення пробілів в назвах стовпців\n",
    "    return concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde1e0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_datetime_index(df: pd.DataFrame, date_map: Dict[str, str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Перетворює стовпець 'Day' у datetime-індекс на основі мапи дат,\n",
    "    встановлює його як індекс і видаляє стовпець 'Day'.\n",
    "\n",
    "    :param df: DataFrame з колонкою 'Day'\n",
    "    :param date_map: словник мітка -> datetime рядок\n",
    "    :return: DataFrame з datetime-індексом\n",
    "    \"\"\"\n",
    "    df['timestamp'] = pd.to_datetime(df['Day'].map(date_map))\n",
    "    df = df.set_index('timestamp').drop(columns=['Day'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68a886e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Додає циклічні ознаки для дня тижня та години (синус/косинус).\n",
    "\n",
    "    :param df: DataFrame з datetime-індексом\n",
    "    :return: DataFrame з новими часовими ознаками\n",
    "    \"\"\"\n",
    "    df['dow'] = df.index.dayofweek  # день тижня (0=Понеділок)\n",
    "    df['hour'] = df.index.hour  # година доби\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "799e270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Очищення даних:\n",
    "      - Негативні числа у числових колонках замінюються на NaN\n",
    "      - Нескінченності замінюються на NaN\n",
    "      - Видалення рядків з будь-якими NaN\n",
    "      - Скидання індексу\n",
    "\n",
    "    :param df: початковий або частково оброблений DataFrame\n",
    "    :return: очищений DataFrame, готовий до аналізу або моделювання\n",
    "    \"\"\"\n",
    "    # Визначення числових колонок (без деяких виключень)\n",
    "    exclude = ['Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'Label']\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.difference(exclude)\n",
    "\n",
    "    # Маска негативних значень та нескінченностей\n",
    "    df[numeric_cols] = df[numeric_cols].mask(df[numeric_cols] < 0)\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Видалення рядків з пропусками\n",
    "    df = df.dropna(axis=0, how='any')\n",
    "\n",
    "    # Скидання індексу\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82931ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = load_and_concat_csvs(DATA_DIR)\n",
    "df_indexed = add_datetime_index(raw_df, DATE_MAP)\n",
    "df_features = engineer_time_features(df_indexed)\n",
    "df_clean = clean_data(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00698d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) log1p: сильна права асиметрія (skew > 1)\n",
    "LOG1P_FEATURES = [\n",
    "    'Active Mean',\n",
    "    'Average Packet Size',\n",
    "    'Bwd Header Length',\n",
    "    'Bwd IAT Mean',\n",
    "    'Bwd IAT Std',\n",
    "    'Down/Up Ratio',\n",
    "    'Flow Bytes/s',\n",
    "    'Flow Duration',\n",
    "    'Flow IAT Mean',\n",
    "    'Flow IAT Min',\n",
    "    'Flow IAT Std',\n",
    "    'Flow Packets/s',\n",
    "    'Fwd Header Length',\n",
    "    'Fwd IAT Mean',\n",
    "    'Fwd IAT Std',\n",
    "    'Fwd PSH Flags',\n",
    "    'Fwd Packets/s',\n",
    "    'Idle Mean',\n",
    "    'Max Packet Length',\n",
    "    'Min Packet Length',\n",
    "    'Packet Length Mean',\n",
    "    'Packet Length Std',\n",
    "    'Packet Length Variance'\n",
    "]\n",
    "\n",
    "# 2) Yeo–Johnson: сильна ліва асиметрія (skew < –1), помірна права асиметрія (0.5 < skew ≤ 1), або наявність нулів/від’ємних\n",
    "YEO_JOHNSON_FEATURES = [\n",
    "    'Total Length of Fwd Packets'\n",
    "    'Fwd Packet Length Max'\n",
    "]\n",
    "\n",
    "# 3) QuantileTransformer: надзвичайно важкі хвости (skew > 50)\n",
    "QUANTILE_FEATURES = [\n",
    "    'Bwd Packets/s',\n",
    "    'Subflow Bwd Packets',\n",
    "    'Subflow Fwd Packets',\n",
    "    'Total Backward Packets',\n",
    "    'Total Fwd Packets'\n",
    "]\n",
    "\n",
    "# 4) За рештою ознак (|skew| ≤ 0.5) достатньо RobustScaler або залишити без трансформації:\n",
    "ROBUST_FEATURES = [\n",
    "    'Bwd Avg Bytes/Bulk',\n",
    "    'Bwd URG Flags',\n",
    "    'Flow IAT Max',\n",
    "    'Fwd Avg Bytes/Bulk',\n",
    "    'Fwd Avg Packets/Bulk',\n",
    "    'Fwd IAT Min',\n",
    "    'Fwd Packet Length Min',\n",
    "]\n",
    "\n",
    "\n",
    "BIN_FEATURES: List[str] = [\n",
    "    'Flow IAT Min', 'Fwd Header Length', 'Fwd PSH Flags',\n",
    "    'Packet Length Variance', 'Total Backward Packets', 'Total Fwd Packets'\n",
    "]\n",
    "\n",
    "DROP_FEATURES: List[str] = [\n",
    "    'Packet Length Std', 'Max Packet Length', 'Average Packet Size',\n",
    "    'Flow IAT Std', 'Fwd IAT Mean', 'Bwd IAT Mean',\n",
    "    'Fwd Header Length', 'Bwd Header Length', 'Label', 'Category', 'dow','hour'\n",
    "]\n",
    "\n",
    "_transformers: Dict[str, object] = {\n",
    "    'log1p': FunctionTransformer(np.log1p, validate=False),\n",
    "    'yeo_johnson': PowerTransformer(method='yeo-johnson', standardize=False),\n",
    "    'quantile': QuantileTransformer(output_distribution='normal',\n",
    "                                    random_state=0),\n",
    "    'robust': RobustScaler(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d9fb1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformers(df: pd.DataFrame,\n",
    "                       feature_map: Dict[str, List[str]],\n",
    "                       transformers: Dict[str, object],\n",
    "                       fit: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Застосовує набір трансформерів до відповідних списків ознак.\n",
    "\n",
    "    Параметри\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Вхідний DataFrame.\n",
    "    feature_map : dict\n",
    "        Відображення ключів трансформерів на списки імен ознак.\n",
    "    transformers : dict\n",
    "        Відображення ключів на інстанси scikit-learn трансформерів.\n",
    "    fit : bool, default=True\n",
    "        Якщо True — спочатку навчає трансформери на даних, інакше — лише трансформує.\n",
    "\n",
    "    Повертає\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame з трансформованими ознаками.\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "    for key, features in feature_map.items():\n",
    "        transformer = transformers[key]\n",
    "        present = [f for f in features if f in df_out.columns]\n",
    "        if not present:\n",
    "            continue\n",
    "        data = df_out[present]\n",
    "        if fit:\n",
    "            df_out.loc[:, present] = transformer.fit_transform(data)\n",
    "        else:\n",
    "            df_out.loc[:, present] = transformer.transform(data)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2918ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_features(df: pd.DataFrame,\n",
    "                 features: List[str],\n",
    "                 n_bins: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Квантільно бінує вибрані неперервні ознаки.\n",
    "\n",
    "    Параметри\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Вхідний DataFrame.\n",
    "    features : list of str\n",
    "        Список імен ознак для бінування.\n",
    "    n_bins : int, default=5\n",
    "        Кількість бінів (квантилів).\n",
    "\n",
    "    Повертає\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Копія DataFrame з новими стовпцями '{feature}_bin'.\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "    for feat in features:\n",
    "        if feat in df_out.columns:\n",
    "            df_out[f'{feat}_bin'] = pd.qcut(\n",
    "                df_out[feat], q=n_bins, labels=False, duplicates='drop'\n",
    "            )\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ba2c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(df: pd.DataFrame,\n",
    "               category_labels: Dict[str, List[str]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Відображає сирі рядки міток у числові коди категорій.\n",
    "\n",
    "    Параметри\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame із стовпцем 'Label'.\n",
    "    category_labels : dict\n",
    "        Відображення категорій на списки сирих міток.\n",
    "\n",
    "    Повертає\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame з доданими стовпцями 'Category' та 'label_code'.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_out = df.copy()\n",
    "    df_out['Label'] = df_out['Label'].str.replace('�', '–', regex=False)\n",
    "\n",
    "    label_to_cat = {\n",
    "        raw: cat\n",
    "        for cat, raws in category_labels.items()\n",
    "        for raw in raws\n",
    "    }\n",
    "    df_out['Category'] = df_out['Label'].map(label_to_cat)\n",
    "\n",
    "    cats = list(category_labels.keys())\n",
    "    code_map = {cat: idx for idx, cat in enumerate(cats)}\n",
    "    df_out['label_code'] = df_out['Category'].map(code_map)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "deb5c25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BENIGN': 0,\n",
       " 'DoS': 1,\n",
       " 'PortScan': 2,\n",
       " 'Bot_Infiltration': 3,\n",
       " 'Web': 4,\n",
       " 'FTP_SSH_Patator': 5,\n",
       " 'Heartbleed': 6}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = list(CATEGORY_LABELS.keys())\n",
    "code_map = {cat: idx for idx, cat in enumerate(cats)}\n",
    "\n",
    "code_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f3cc72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Комбінує день тижня та годину в один індекс часу.\n",
    "\n",
    "    Параметри\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame зі стовпцями 'dow' та 'hour'.\n",
    "\n",
    "    Повертає\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame з новим стовпцем 'time'.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_out = df.copy()\n",
    "    df_out['time'] = df_out['dow'] * 24 + df_out['hour']\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e75c8766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_and_select(df: pd.DataFrame,\n",
    "                    drop_cols: List[str],\n",
    "                    group_features: Dict[str, List[str]],\n",
    "                    base_features: List[str],\n",
    "                    std_features: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Видаляє небажані стовпці та вибирає лише існуючі в заданому порядку.\n",
    "\n",
    "    Параметри\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Вхідний DataFrame.\n",
    "    drop_cols : list of str\n",
    "        Список стовпців для видалення.\n",
    "    group_features : dict\n",
    "        Групи ознак для агрегації.\n",
    "    base_features : list of str\n",
    "        Базовий список ознак.\n",
    "    std_features : list of str\n",
    "        Додатковий список ознак.\n",
    "\n",
    "    Повертає\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Відфільтрований DataFrame.\n",
    "    \"\"\"\n",
    "    df_out = df.drop(columns=drop_cols, errors='ignore').copy()\n",
    "    # Flatten group columns and preserve order without duplicates\n",
    "    group_cols = [c for cols in group_features.values() for c in cols]\n",
    "    all_feats = list(OrderedDict.fromkeys(base_features + std_features + group_cols))\n",
    "    existing = [c for c in all_feats if c in df_out.columns]\n",
    "    # Keep label_code and composite if present\n",
    "    for extra in ['label_code', 'composite']:\n",
    "        if extra in df_out.columns:\n",
    "            existing.append(extra)\n",
    "    return df_out[existing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "069ffe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_balance(df: pd.DataFrame,\n",
    "                      test_size: float,\n",
    "                      val_split: float,\n",
    "                      random_state: int,\n",
    "                      smote_state: int) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Розбиває дані на тренувальний, валідаційний та тестовий набори,\n",
    "    стратифікує за часовим композитом та застосовує SMOTE-балансування.\n",
    "\n",
    "    Параметри\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame з 'label_code' та 'composite'.\n",
    "    test_size : float\n",
    "        Частка для тестового набору.\n",
    "    val_split : float\n",
    "        Частка від залишку для валідації.\n",
    "    random_state : int\n",
    "        Насіння для відтворюваності.\n",
    "    smote_state : int\n",
    "        Насіння SMOTE.\n",
    "\n",
    "    Повертає\n",
    "    -------\n",
    "    dict of pandas.DataFrame\n",
    "        Словник із ключами 'X_train_bal', 'y_train_bal', 'X_val', 'y_val',\n",
    "        'X_test', 'y_test'.\n",
    "    \"\"\"\n",
    "    X = df.drop(columns=['label_code', 'composite'])\n",
    "    y = df['label_code']\n",
    "    c = df['composite']\n",
    "\n",
    "    # First split off test\n",
    "    X_temp, X_test, y_temp, y_test, c_temp, c_test = train_test_split(\n",
    "        X, y, c, test_size=test_size,\n",
    "        stratify=c, random_state=random_state\n",
    "    )\n",
    "    # Then split temp into train/val\n",
    "    X_train, X_val, y_train, y_val, c_train, c_val = train_test_split(\n",
    "        X_temp, y_temp, c_temp, test_size=val_split,\n",
    "        stratify=c_temp, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Determine SMOTE k_neighbors\n",
    "    min_count = y_train.value_counts().min()\n",
    "    k = max(1, min(min_count - 1, 5))\n",
    "    print(f\"SMOTE will use k_neighbors={k} (min class count = {min_count})\")\n",
    "\n",
    "    sm = SMOTE(k_neighbors=k, random_state=smote_state)\n",
    "    X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Debug prints\n",
    "    print(\"TRAIN classes:\", Counter(y_train_bal))\n",
    "    print(\"VAL   classes:\", Counter(y_val))\n",
    "    print(\"TEST  classes:\", Counter(y_test))\n",
    "    print(\"TRAIN times:\", Counter(c_train % 168))\n",
    "    print(\"VAL   times:\", Counter(c_val % 168))\n",
    "    print(\"TEST  times:\", Counter(c_test % 168))\n",
    "\n",
    "    return {\n",
    "        'X_train_bal': X_train_bal, 'y_train_bal': y_train_bal,\n",
    "        'X_val': X_val,          'y_val': y_val,\n",
    "        'X_test': X_test,        'y_test': y_test,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30ac0f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_datasets(datasets: Dict[str, pd.DataFrame],\n",
    "                    base_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Зберігає набори даних у CSV-файли під вказаним базовим шляхом.\n",
    "\n",
    "    Параметри\n",
    "    ----------\n",
    "    datasets : dict\n",
    "        Словник з іменами наборів даних та DataFrame.\n",
    "    base_path : str\n",
    "        Базовий шлях для збереження файлів.\n",
    "\n",
    "    Повертає\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    for name, df in datasets.items():\n",
    "        filename = f\"{base_path}/{name}.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Saved {name} to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc6ed383",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOTE_RANDOM_STATE = 1\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SPLIT = 0.12\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5cf94fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dubst\\AppData\\Local\\Temp\\ipykernel_3172\\1695121636.py:32: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.69314718 0.69314718 0.69314718 ... 0.         0.         0.        ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_out.loc[:, present] = transformer.fit_transform(data)\n",
      "C:\\Users\\dubst\\AppData\\Local\\Temp\\ipykernel_3172\\1695121636.py:32: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.         0.         0.         ... 0.69314718 0.         0.        ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_out.loc[:, present] = transformer.fit_transform(data)\n",
      "C:\\Users\\dubst\\AppData\\Local\\Temp\\ipykernel_3172\\1695121636.py:32: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[6.88448665 6.88448665 0.         ... 3.4657359  4.8598124  4.73619845]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_out.loc[:, present] = transformer.fit_transform(data)\n",
      "C:\\Users\\dubst\\AppData\\Local\\Temp\\ipykernel_3172\\1695121636.py:32: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.         0.         0.         ... 0.         3.49650756 3.87120101]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_out.loc[:, present] = transformer.fit_transform(data)\n",
      "C:\\Users\\dubst\\AppData\\Local\\Temp\\ipykernel_3172\\1695121636.py:32: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 2.21112724  2.21112724 -0.51635627 ... -0.51635627  0.20337897\n",
      "  0.20337897]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_out.loc[:, present] = transformer.fit_transform(data)\n",
      "C:\\Users\\dubst\\AppData\\Local\\Temp\\ipykernel_3172\\1695121636.py:32: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 2.25674483  2.25674483 -5.19933758 ... -0.15239059  0.81614989\n",
      "  0.59244364]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_out.loc[:, present] = transformer.fit_transform(data)\n",
      "C:\\Users\\dubst\\AppData\\Local\\Temp\\ipykernel_3172\\1695121636.py:32: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 2.21112724  2.21112724 -0.51635627 ... -0.51635627  0.20337897\n",
      "  0.20337897]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_out.loc[:, present] = transformer.fit_transform(data)\n",
      "C:\\Users\\dubst\\AppData\\Local\\Temp\\ipykernel_3172\\1695121636.py:32: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 2.25674483  2.25674483 -5.19933758 ... -0.15239059  0.81614989\n",
      "  0.59244364]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_out.loc[:, present] = transformer.fit_transform(data)\n",
      "C:\\Users\\dubst\\AppData\\Local\\Temp\\ipykernel_3172\\1695121636.py:32: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.05555556 -0.05555556 -0.05555556 ... -0.05555556  0.83333333\n",
      "  1.25      ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_out.loc[:, present] = transformer.fit_transform(data)\n"
     ]
    }
   ],
   "source": [
    "df_transformed = apply_transformers(\n",
    "    df_clean,\n",
    "    feature_map={\n",
    "        'log1p': LOG1P_FEATURES,\n",
    "        'yeo_johnson': YEO_JOHNSON_FEATURES,\n",
    "        'quantile': QUANTILE_FEATURES,\n",
    "        'robust': ROBUST_FEATURES,\n",
    "    },\n",
    "    transformers=_transformers,\n",
    "    fit=True\n",
    ")\n",
    "\n",
    "df_binned = bin_features(df_transformed, BIN_FEATURES)\n",
    "df_mapped = map_labels(df_binned, CATEGORY_LABELS)\n",
    "df_time = add_time_feature(df_mapped)\n",
    "df_time['composite'] = df_time['label_code'] * 168 + df_time['time']\n",
    "df_final = drop_and_select(\n",
    "    df_time, DROP_FEATURES, GROUP_FEATURES, BASE_FEATURES, STD_FEATURES\n",
    ")\n",
    "\n",
    "data_dict = split_and_balance(\n",
    "    df_final, TEST_SIZE, VAL_SPLIT, RANDOM_STATE, SMOTE_RANDOM_STATE\n",
    ")\n",
    "export_datasets(data_dict, '../NN Datasets')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
